"Question","Question Type","Answer Option 1","Explanation 1","Answer Option 2","Explanation 2","Answer Option 3","Explanation 3","Answer Option 4","Explanation 4","Answer Option 5","Explanation 5","Answer Option 6","Explanation 6","Correct Answers","Overall Explanation","Domain"
"Scenario: Executive Services, Inc. is a Headhunter agency whose CEO is Lenny Rollie. Their main office is in Manhattan, New York, and has been wildly successful in placing top-level executives at many key companies around the world. Lenny has approached you to assist ESI in their internal IT projects and has asked you to advise the IT team on the following matter.
The current project that you are consulting uses Azure Machine Learning service and Lenny wants to expand their use of machine learning.
ESI has the following compute environments.
The organization does not want to create another compute environment.
[IMAGE: word-image-43535-354.png]
Lenny has asked you to assist the IT team in determining which compute environment for the following case:
• Run an Azure Machine Learning Designer training pipeline
Which compute type should be used?","multiple-choice","aks_cluster","","nb_server","","Any of the options will work equally well","","None of the options will work","","mlc_cluster","","","","5","Correct
To run an Azure Machine Learning Designer training pipeline the best choice is mlc_cluster.
For AML Studio, the training pipeline can be either AML Compute or Compute Instance. The inferencing can only be Azure Kubernetes Cluster.
A compute target is a designated compute resource or environment where you run your training script or host your service deployment. This location might be your local machine or a cloud-based compute resource. Using compute targets makes it easy for you to later change your compute environment without having to change your code.
In a typical model development lifecycle, you might:
1.Start by developing and experimenting on a small amount of data. At this stage, use your local environment, such as a local computer or cloud-based virtual machine (VM), as your compute target.
2.Scale up to larger data, or do distributed training by using one of these training compute targets.
3.After your model is ready, deploy it to a web hosting environment or IoT device with one of these deployment compute targets.
The compute resources you use for your compute targets are attached to a workspace. Compute resources other than the local machine are shared by users of the workspace.
Training compute targets
Azure Machine Learning has varying support across different compute targets. A typical model development lifecycle starts with development or experimentation on a small amount of data. At this stage, use a local environment like your local computer or a cloud-based VM. As you scale up your training on larger datasets or perform distributed training, use Azure Machine Learning compute to create a single- or multi-node cluster that autoscales each time you submit a run. You can also attach your own compute resource, although support for different scenarios might vary.
Compute targets can be reused from one training job to the next. For example, after you attach a remote VM to your workspace, you can reuse it for multiple jobs. For machine learning pipelines, use the appropriate pipeline step for each compute target.
You can use any of the following resources for a training compute target for most jobs. Not all resources can be used for automated machine learning, machine learning pipelines, or designer.
[IMAGE: word-image-43535-355.png]
Compute targets for inference
When performing inference, Azure Machine Learning creates a Docker container that hosts the model and associated resources needed to use it. This container is then used in a compute target.
[IMAGE: word-image-43535-356.png]
The compute target you use to host your model will affect the cost and availability of your deployed endpoint. Use this table to choose an appropriate compute target.
https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer#compute",""
"Scenario: Pym Tech is a U.S.-based Technology manufacturer headed by Hank Pym. Their headquarters is located at Treasure Island, San Francisco California and business is booming.
The expansion plans are underway which have presented several IT challenges which Hank has contracted you to advise his IT staff on.
At the moment, the topic is creating a new Azure Machine Learning pipeline using the designer.
Required:
• The pipeline must train a model using data in a comma-separated value (CSV) file that is published on a website.
• Must ingest the data from the CSV file into the designer pipeline using minimal administrative effort.
Given:
A dataset has not been created for this file.
Which module should be added to the pipeline in Designer?","multiple-choice","Convert to CSV","","Dataset","","Import Data","","Enter Data Manually","","","","","","3","Correct
The team should add Import Data to the pipeline in Designer.
There are two ways you can import data into the designer:
• Azure Machine Learning datasets – Register datasets in Azure Machine Learning to enable advanced features that help you manage your data.
• Import Data module – Use the Import Data module to directly access data from online datasources.
The Import Data module support read data from following sources:
• URL via HTTP
• Azure cloud storages through Datastores)
•Azure Blob Container
•Azure File Share
•Azure Data Lake
•Azure Data Lake Gen2
•Azure SQL Database
•Azure PostgreSQL
Before using cloud storage, you need to register a datastore in your Azure Machine Learning workspace first.
After you define the data you want and connect to the source, Import Data infers the data type of each column based on the values it contains, and loads the data into your designer pipeline. The output of Import Data is a dataset that can be used with any designer pipeline.
If your source data changes, you can refresh the dataset and add new data by rerunning Import Data.
Note: If your workspace is in a virtual network, you must configure your datastores to use the designer‘s data visualization features.
https://docs.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/import-data
MS recommends that you use datasets to import data into the designer. When you register a dataset, you can take full advantage of advanced data features like versioning and tracking and data monitoring.
https://docs.microsoft.com/en-us/azure/machine-learning/how-to-designer-import-data",""
"Scenario: The UCWF (Unlimited Class Wrestling Federation) was founded by a promoter named Edward Garner, who strives to use technology to improve his business and has come to you for assistance with the company’s Microsoft Azure service.
At this moment, the team is solving a classification task and must evaluate the current model on a limited data sample by using k-fold cross-validation.
The data scientist starts by configuring a k parameter as the number of splits and needs to configure the k parameter for the cross-validation.
Which of the following values should they use?","multiple-choice","k=1","","k=0.9","","k=10","","k=0.5","","","","","","3","Correct
The data scientist should use k=10 for the k parameter for the cross-validation.
Leave One Out (LOO) cross-validation
Setting K = n (the number of observations) yields n-fold and is called leave-one out cross-validation (LOO), a special case of the K-fold approach.
LOO CV is sometimes useful but typically doesn’t shake up the data enough. The estimates from each fold are highly correlated and hence their average can have high variance.
The choice of k is usually 5 or 10, but there is no formal rule. As k gets larger, the difference in size between the training set and the resampling subsets gets smaller. As this difference decreases, the bias of the technique becomes smaller.
K=5 or 10 provides a good compromise for the bias-variance trade-off.
k-Fold Cross-Validation
Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.
The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.
Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.
It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.
The general procedure is as follows:
1. Shuffle the dataset randomly.
2. Split the dataset into k groups
3. For each unique group:
i.Take the group as a hold out or test data set
ii.Take the remaining groups as a training data set
iii.Fit a model on the training set and evaluate it on the test set
iv.Retain the evaluation score and discard the model
4. Summarize the skill of the model using the sample of model evaluation scores
Important: Each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the hold out set 1 time and used to train the model k-1 times.
https://machinelearningmastery.com/k-fold-cross-validation/",""
"Scenario: The UCWF (Unlimited Class Wrestling Federation) was founded by a promoter named Edward Garner, who strives to use technology to improve his business and has come to you for assistance with the company’s Microsoft Azure service.
At this moment, the team is creating a new experiment in Azure Machine Learning Designer and has a small dataset that has missing values in many columns. The data does not require the application of predictors for each column.
The developer plans to use the Clean Missing Data and must select a data cleaning method.
Below are the options the developer is considering. Which method should they use?","multiple-choice","Replace using Probabilistic PCA","","Replace using MICE","","Normalization","","Synthetic Minority Oversampling Technique (SMOTE)","","","","","","1","Correct
The developer should use the method: Replace using Probabilistic PCA.
Replace using Probabilistic PCA: Compared to other options, such as Multiple Imputation using Chained Equations (MICE), this option has the advantage of not requiring the application of predictors for each column. Instead, it approximates the covariance for the full dataset. Therefore, it might offer better performance for datasets that have missing values in many columns.
Data scientists often check data for missing values and then perform various operations to fix the data or insert new values. The goal of such cleaning operations is to prevent problems caused by missing data that can arise when training a model.
The Clean Missing Data module supports multiple type of operations for “cleaning“ missing values, including:
• Replacing missing values with a placeholder, mean, or other value
• Completely removing rows and columns that have missing values
• Inferring values based on statistical methods
Using this module does not change your source dataset. Instead, it creates a new dataset in your workspace that you can use in the subsequent workflow. You can also save the new, cleaned dataset for reuse.
The Clean Missing Data module also outputs a definition of the transformation used to clean the missing values. You can re-use this transformation on other datasets that have the same schema, by using the Apply Transformation module.
The Clean Missing Data module lets you define a cleaning operation. You can also save the cleaning operation so that you can apply it later to new data.
• Replace missing values
• Apply a cleaning transformation to new data
https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/clean-missing-data",""
"Identify the missing word(s) in the following sentence within the context of Microsoft Azure.
Before exploring how to train a deep neural network (DNN) machine learning model, one should consider what they are trying to achieve.
Consider the following image:
[IMAGE: word-image-43535-357.png]
The deep neural network model for the classifier consists of multiple layers of artificial neurons. Because of the layered architecture of the network, this kind of model is sometimes referred to as a multilayer perceptron.
The training process for a deep neural network consists of multiple iterations, called [].","multiple-choice","Optimizers","","Batches","","Derivatives","","Epochs","","","","","","4","Correct
The training process for a deep neural network consists of multiple iterations, called epochs.
Before exploring how to train a deep neural network (DNN) machine learning model, one should consider what they are trying to achieve. Machine learning is concerned with predicting a label based on some features of a particular observation. In simple terms, a machine learning model is a function that calculates y (the label) from x (the features): f(x)=y.
A simple classification example
For example, suppose our observation consists of some measurements of a penguin.
[IMAGE: word-image-43535-358.png]
Specifically, the measurements are:
• The length of the penguin‘s bill.
• The depth of the penguin‘s bill.
• The length of the penguin‘s flipper.
• The penguin‘s weight.
In this case, the features (x) are a vector of four values, or mathematically, x=[x1,x2,x3,x4].
Let‘s suppose that the label we‘re trying to predict (y) is the species of the penguin, and that there are three possible species it could be:
1. Adelie
2. Gentoo
3. Chinstrap
This is an example of a classification problem, in which the machine learning model must predict the most probable class to which the observation belongs. A classification model accomplishes this by predicting a label that consists of the probability for each class. In other words, y is a vector of three probability values; one for each of the possible classes: y=[P(0),P(1),P(2)].
We train the machine learning model by using observations for which we already know the true label. For example, we may have the following feature measurements for an Adelie specimen:
x=[37.3, 16.8, 19.2, 30.0]
We already know that this is an example of an Adelie (class 0), so a perfect classification function should result in a label that indicates a 100% probability for class 0, and a 0% probability for classes 1 and 2:
y=[1, 0, 0]
A deep neural network model
So how would we use deep learning to build a classification model for the penguin classification model? Let‘s look at an example:
[IMAGE: word-image-43535-359.png]
The deep neural network model for the classifier consists of multiple layers of artificial neurons. In this case, there are four layers:
• An input layer with a neuron for each expected input (x) value.
• Two so-called hidden layers, each containing five neurons.
• An output layer containing three neurons – one for each class probability (y) value to be predicted by the model.
Because of the layered architecture of the network, this kind of model is sometimes referred to as a multilayer perceptron. Additionally, notice that all neurons in the input and hidden layers are connected to all neurons in the subsequent layers – this is an example of a fully connected network.
When we create a model like this, we must define an input layer that supports the number of features our model will process, and an output layer that reflects the number of outputs we expect it to produce. We can decide how many hidden layers we want to include and how many neurons are in each of them; but we have no control over the input and output values for these layers – these are determined by the model training process.
Training a deep neural network
The training process for a deep neural network consists of multiple iterations, called epochs. For the first epoch, we start by assigning random initialization values for the weight (w) and bias b values. Then the process is as follows:
1. Features for data observations with known label values are submitted to the input layer. Generally, these observations are grouped into batches (often referred to as mini-batches).
2. The neurons then apply their function, and if activated, pass the result onto the next layer until the output layer produces a prediction.
3. The prediction is compared to the actual known value, and the amount of variance between the predicted and true values (which we call the loss) is calculated.
4. Based on the results, revised values for the weights and bias values are calculated to reduce the loss, and these adjustments are backpropagated to the neurons in the network layers.
5. The next epoch repeats the batch training forward pass with the revised weight and bias values, hopefully improving the accuracy of the model (by reducing the loss).
Note: Processing the training features as a batch improves the efficiency of the training process by processing multiple observations simultaneously as a matrix of features with vectors of weights and biases. Linear algebraic functions that operate with matrices and vectors also feature in 3D graphics processing, which is why computers with graphic processing units (GPUs) provide significantly better performance for deep learning model training than central processing unit (CPU) only computers.
A closer look at loss functions and backpropagation
The previous description of the deep learning training process mentioned that the loss from the model is calculated and used to adjust the weight and bias values. How exactly does this work?
Calculating loss
Suppose one of the samples passed through the training process contains features of an Adelie specimen (class 0). The correct output from the network would be [1, 0, 0]. Now suppose that the output produced by the network is [0.4, 0.3, 0.3]. Comparing these, we can calculate an absolute variance for each element (in other words, how far is each predicted value away from what it should be) as [0.6, 0.3, 0.3].
In reality, since we‘re actually dealing with multiple observations, we typically aggregate the variance – for example by squaring the individual variance values and calculating the mean, so we end up with a single, average loss value, like 0.18.
Optimizers
Now, here‘s the clever bit. The loss is calculated using a function, which operates on the results from the final layer of the network, which is also a function. The final layer of network operates on the outputs from the previous layers, which are also functions. So in effect, the entire model from the input layer right through to the loss calculation is just one big nested function. Functions have a few really useful characteristics, including:
• We can conceptualize a function as a plotted line comparing its output with each of its variables.
• We can use differential calculus to calculate the derivative of the function at any point with respect to its variables.
Let‘s take the first of these capabilities. We can plot the line of the function to show how an individual weight value compares to loss, and mark on that line the point where the current weight value matches the current loss value.
[IMAGE: word-image-43535-360.png]
Now let‘s apply the second characteristic of a function. The derivative of a function for a given point indicates whether the slope (or gradient) of the function output (in this case, loss) is increasing or decreasing with respect to a function variable (in this case, the weight value). A positive derivative indicates that the function is increasing, and a negative derivative indicates that it is decreasing. In this case, at the plotted point for the current weight value, the function has a downward gradient. In other words, increasing the weight will have the effect of decreasing the loss.
We use an optimizer to apply this same trick for all of the weight and bias variables in the model and determine in which direction we need to adjust them (up or down) to reduce the overall amount of loss in the model. There are multiple commonly used optimization algorithms, including stochastic gradient descent (SGD), Adaptive Learning Rate (ADADELTA), Adaptive Momentum Estimation (Adam), and others; all of which are designed to figure out how to adjust the weights and biases to minimize loss.
Learning rate
Now, the obvious next question is, by how much should the optimizer adjust the weights and bias values? If we look at the plot for our weight value, we can see that increasing the weight by a small amount will follow the function line down (reducing the loss), but if we increase it by too much, the function line starts to go up again, so we might actually increase the loss; and after the next epoch, we might find we need to reduce the weight.
The size of the adjustment is controlled by a parameter that we set for training called the learning rate. A low learning rate results in small adjustments (so it can take more epochs to minimize the loss), while a high learning rate results in large adjustments (so we might miss the minimum altogether).
https://docs.microsoft.com/en-us/archive/msdn-magazine/2018/february/machine-learning-deep-neural-network-classifiers-using-cntk",""
"Before you begin training models, it‘s essential to secure your Azure Machine Learning network from outside intrusion. Without first securing your network, you can leave your data and models exposed to potentially malicious actors and lead to data theft or attacks that could negatively change model behaviour.
To begin securing your network, you will need to connect to your workspace via a private endpoint (private IP), which we will cover in the next exercise. The private endpoint can be added to a workspace through the Azure Machine Learning Python SDK, Azure CLI, or within the Networking tab of the Azure portal. From there, you can then limit access to your workspace to only occur over the private IP addresses.
Integrating Azure services to an Azure virtual network enables private access to the service from virtual machines or compute resources in the virtual network.
Which of the following options can be used to integrate Azure services in your virtual network (Select two)","multi-select","Azure Virtual Desktop endpoints","","Private endpoints","","Microsoft Remote Desktop endpoints","","Service endpoints","","SecureRoute endpoints","","","","2,4","Correct
You can integrate Azure services in your virtual network with the following options:
Service endpoints provide the identity of your virtual network to the Azure service. Once you enable service endpoints in your virtual network, you can add a virtual network rule to secure the Azure service resources to your virtual network. Service endpoints use public IP addresses.
Private endpoints are network interfaces that securely connect you to a service powered by Azure Private Link. Private endpoint uses a private IP address from your VNet, effectively bringing the Azure services into your VNet.
Secure your Azure Machine Learning network
Before you begin training models, it‘s essential to secure your Azure Machine Learning network from outside intrusion. Without first securing your network, you can leave your data and models exposed to potentially malicious actors and lead to data theft or attacks that could negatively change model behaviour. These alterations can often be difficult to spot due to the often large nature of datasets or parameters influencing model behaviour. We will begin by separating your model training from the wider net to its own virtual network to avoid these problems.
Virtual networks and security groups
To secure the Azure Machine Learning workspace and compute resources, we will use a virtual network (VNet). An Azure VNet is the fundamental building block for your private network in Azure. VNet enables Azure resources, such as Azure Blob Storage and Azure Container Registry, to securely communicate with each other, the internet, and on-premises networks. VNet is similar to a traditional network that you‘d operate in your own data centre, but brings with it additional benefits of Azure‘s infrastructure such as scale, availability, and isolation. With a VNet, you can enhance security between Azure resources and filter network traffic to ensure only trusted users have access to the network.
[IMAGE: word-image-43535-361.png]
In the above image, we can see a typical structure for a Virtual Network (VNet) comprised of:
IP address space: When creating a VNet, you must specify a custom private IP address space using public and private (RFC 1918) addresses.
Subnets: Shown above as separate virtual machines (VMs), subnets enable you to segment the virtual network into one or more sub-networks and allocate a portion of the virtual network‘s address space to each subnet, enhancing security and performance.
Network interfaces (NIC) are the interconnection between a VM and a virtual network (VNet). When you create a VM in the Azure portal, a network interface is automatically created for you.
Network security groups (NSG) can contain multiple inbound and outbound security rules that enable you to filter traffic to and from resources by source and destination IP address, port, and protocol.
Load balancers can be configured to efficiently handle inbound and outbound traffic to VMs and VNets, while also offering metrics to monitor the health of VMs.
Paths into a VNet
Integrating Azure services to an Azure virtual network enables private access to the service from virtual machines or compute resources in the virtual network. You can integrate Azure services in your virtual network with the following options:
Service endpoints provide the identity of your virtual network to the Azure service. Once you enable service endpoints in your virtual network, you can add a virtual network rule to secure the Azure service resources to your virtual network. Service endpoints use public IP addresses.
Private endpoints are network interfaces that securely connect you to a service powered by Azure Private Link. Private endpoint uses a private IP address from your VNet, effectively bringing the Azure services into your VNet.
[IMAGE: word-image-43535-362.png]
You can connect your on-premises computers and networks to a VNet through a virtual private network (VPN) in several ways. A Point-to-site VPN is a connection between a virtual network and a single computer in your network. The communication is sent through an encrypted tunnel over the internet. Each computer that wants to establish connectivity with a VNet must configure its connection, so it‘s best used if you only have a few users who need to connect to the VNet. This connection type is great if you‘re just getting started as it requires little or no changes to your existing network.
While a Site-to-site VPN can be established between your on-premises VPN device and an Azure VPN Gateway that‘s deployed in a virtual network. This connection type enables any on-premises resource that you authorize to access a virtual network. The communication between your on-premises VPN device and an Azure VPN gateway is sent through an encrypted tunnel over the internet.
ExpressRoute can be used instead of a VPN If you wish to speed up creating private connections to Azure services. The service allows you to create private connections between Microsoft data centers and infrastructure on your premises or in another facility. ExpressRoute connections are separate from the public internet and offer high security, reliability, and speeds with lower latency than typical connections over the internet. ExpressRoute has a range of pricing options depending on your estimated bandwidth requirements.
Securing the workspace and resources
To begin securing your network, you will need to connect to your workspace via a private endpoint (private IP), which we will cover in the next exercise. The private endpoint can be added to a workspace through the Azure Machine Learning Python SDK, Azure CLI, or within the Networking tab of the Azure portal. From there, you can then limit access to your workspace to only occur over the private IP addresses.
However, this alone will not ensure end-to-end security by itself, so make sure other Azure services you are communicating with are also behind the VNet. Since communication to the workspace is then set to be only allowed from the VNet, any development environments that use the workspace must be members of the VNet unless you have configured the network to allow public IP connections.
The following methods can be used to connect to the secure workspace:
Azure VPN gateway – Connects on-premises networks to the VNet over a private connection. Connection is made over the public internet. There are two types of VPN gateways that you might use:
Point-to-site: Each client computer uses a VPN client to connect to the VNet.
Site-to-site: A VPN device connects the VNet to your on-premises network.
ExpressRoute – Connects on-premises networks into the cloud over a private connection. Connection is made using a connectivity provider.
Azure Bastion – In this scenario, you create an Azure Virtual Machine (sometimes called a jump box) inside the VNet. You then connect to the VM using Azure Bastion. Bastion allows you to connect to the VM using either an RDP or SSH session from your local web browser. You then use the jump box as your development environment. Since it is inside the VNet, it can directly access the workspace.
Security groups and traffic
You can use an Azure network security group (NSG) to filter network traffic to and from Azure resources in an Azure virtual network. A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. NSGs are useful for controlling the traffic flow between VM subnets or limiting outbound communication by resources within an Azure VNet to the internet, which is enabled by default.
The following picture illustrates different scenarios for how network security groups might be deployed to allow network traffic to and from the internet over TCP port 80:
[IMAGE: word-image-43535-363.png]
Application security groups (ASG) can also be used to configure network security as a natural extension of an application‘s structure, allowing you to group virtual machines and define network security policies based on those groups. You can reuse your security policy at scale without manual maintenance of explicit IP addresses. The platform handles the complexity of explicit IP addresses and multiple rule sets, simplifying the NSG rule definition process immensely.
Service tags
A service tag represents a group of IP address prefixes from a given Azure service. Microsoft manages the address prefixes encompassed by the service tag and automatically updates the service tag as addresses change, minimizing the complexity of frequent updates to network security rules.
You can use service tags in place of specific IP addresses when you create security rules to define network access controls on network security groups or Azure Firewall. By specifying the service tag name, such as ApiManagement, in the appropriate source or destination field of a rule, you can allow or deny the traffic for the corresponding service.
[IMAGE: word-image-43535-364.png]
Private endpoints & Private Link
The Azure Machine Learning workspace can use Azure Private Link to create a private endpoint behind the VNet. Azure Private Link is a technology designed to provide connectivity to selected PaaS services. This provides a set of private IP addresses that can be used to access the workspace from within the VNet. Some of the services that Azure Machine Learning relies on can also use Azure Private Link, but some rely on network security groups or user-defined routing.
There are two key components of Azure Private Link:
Private endpoint – a network interface connected to your virtual network, assigned with a private IP address. It is used to connect privately and securely to a service powered by Azure Private Link or a Private Link Service that you or a partner might own.
Private Link Service – your own service, powered by Azure Private Link that runs behind an Azure Standard Load Balancer, enabled for Private Link access. This service can be privately connected with and consumed using Private Endpoints deployed in the user‘s virtual network.
When you create a private endpoint for your Azure resource, it provides secure connectivity between clients on your virtual network and your Azure resource. You can use private endpoints to communicate and ingress events directly from your virtual network to Azure resources securely over a private link without going through the public internet, boosting security. The private endpoint is assigned an IP address from the IP address range of your virtual network. The service is flexible, allowing connections between VNets with overlapping address spaces and connecting resources running in other regions, offering global reach.
[IMAGE: word-image-43535-365.png]
https://learn.microsoft.com/en-us/azure/virtual-network/network-security-groups-overview",""
"Scenario: The Brand Corporation is the science and research branch of the Roxxon Corporation which is managed by Melinda May and Phil Coulson. Melinda and Phil have decided to use Azure for the company to increase its efficiencies and security. Melinda hired you as an advisor to guide many projects to ensure their success.
The research team has the following patient data, which consists of a single feature (blood-glucose level) and a class label 0 for non-diabetic, 1 for diabetic.
[IMAGE: word-image-43535-366.png]
Given: The first eight observations are used to train a classification model, and starting with plotting the blood-glucose feature (x) and the predicted diabetic label (y).
[IMAGE: word-image-43535-367.png]
Which of the following can be used to calculate a probability value for y based on x?","multiple-choice","f(y) = x","","∑(xi−x¯)2y","","f²∑xy","","f(x) = y","","x¯=(∑xN)y","","","","4","Correct
What we need is a function that calculates a probability value for y based on x (in other words, we need the function f(x) = y). You can see from the chart that patients with a low blood-glucose level are all non-diabetic, while patients with a higher blood-glucose level are diabetic. It seems like the higher the blood-glucose level, the more probable it is that a patient is diabetic, with the inflexion point being somewhere between 100 and 110. We need to fit a function that calculates a value between 0 and 1 for y to these values.
Binary classification is classification with two categories. For example, we could label patients as non-diabetic or diabetic.
The class prediction is made by determining the probability for each possible class as a value between 0 -impossible – and 1 – certain. The total probability for all classes is 1, as the patient is definitely either diabetic or non-diabetic. So, if the predicted probability of a patient being diabetic is 0.3, then there is a corresponding probability of 0.7 that the patient is non-diabetic.
A threshold value, usually 0.5, is used to determine the predicted class – so if the positive class (in this case, diabetic) has a predicted probability greater than the threshold, then a classification of diabetic is predicted.
Training and evaluating a classification model
Classification is an example of a supervised machine learning technique, which means it relies on data that includes known feature values (for example, diagnostic measurements for patients) as well as known label values (for example, a classification of non-diabetic or diabetic). A classification algorithm is used to fit a subset of the data to a function that can calculate the probability for each class label from the feature values. The remaining data is used to evaluate the model by comparing the predictions it generates from the features to the known class labels.
A simple example
Let‘s explore a simple example to help explain the key principles. Suppose we have the following patient data, which consists of a single feature (blood-glucose level) and a class label 0 for non-diabetic, 1 for diabetic.
[IMAGE: word-image-43535-368.png]
We‘ll use the first eight observations to train a classification model, and we‘ll start by plotting the blood-glucose feature (which we‘ll call x) and the predicted diabetic label (which we‘ll call y).
[IMAGE: word-image-43535-369.png]
What we need is a function that calculates a probability value for y based on x (in other words, we need the function f(x) = y). You can see from the chart that patients with a low blood-glucose level are all non-diabetic, while patients with a higher blood-glucose level are diabetic. It seems like the higher the blood-glucose level, the more probable it is that a patient is diabetic, with the inflexion point being somewhere between 100 and 110. We need to fit a function that calculates a value between 0 and 1 for y to these values.
One such function is a logistic function, which forms a sigmoidal (S-shaped) curve, like this:
[IMAGE: word-image-43535-370.png]
Now we can use the function to calculate a probability value that y is positive, meaning the patient is diabetic, from any value of x by finding the point on the function line for x. We can set a threshold value of 0.5 as the cut-off point for the class label prediction.
Let‘s test it with the data values we held-back:
[IMAGE: word-image-43535-371.png]
Points plotted below the threshold line will yield a predicted class of 0 – non-diabetic – and points above the line will be predicted as 1 – diabetic.
Now we can compare the label predictions based on the logistic function encapsulated in the model (which we‘ll call ŷ, or “y-hat“) to the actual class labels (y).
[IMAGE: word-image-43535-372.png]
https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-first-experiment-automated-ml",""
"Scenario: The Daily Bugle is a news organization led by J. Jonah Jameson which has a meagre beginning and now has become a household name in news reporting. The company has grown well, despite some technical issues and now Jonah has hired you as an IT consultant to advise on several IT projects geared to improving the company’s efficiencies.
One of the current projects is building a regression model for estimating the number of calls during an event.
Required: Determine whether the feature values achieve the conditions to build a Poisson regression model.
Which of the following conditions must the feature set contain?","multi-select","The label data must be non-discrete.","","The label data must be whole numbers.","","The label data can be positive or negative.","","The label data must be a negative value.","","The label data must be a positive value.","","","","2,5","Correct
Poisson regression is intended for use in regression models that are used to predict numeric values, typically counts. Therefore, you should use this module to create your regression model only if the values you are trying to predict fit the following conditions:
• The response variable has a Poisson distribution.
• Counts cannot be negative. The method will fail outright if you attempt to use it with negative labels.
• A Poisson distribution is a discrete distribution; therefore, it is not meaningful to use this method with non-whole numbers.
Note: If your target isn’t a count, Poisson regression is probably not an appropriate method.
After you have set up the regression method, you must train the model using a dataset containing examples of the value you want to predict. The trained model can then be used to make predictions.
Poisson regression is a special type of regression analysis that is typically used to model counts. For example, Poisson regression would be useful in these scenarios:
• Modeling the number of colds associated with airplane flights
• Estimating the number of emergency service calls during an event
• Projecting the number of customer inquiries subsequent to a promotion
• Creating contingency tables
Because the response variable has a Poisson distribution, the model makes different assumptions about the data and its probability distribution than, say, least-squares regression. Therefore, Poisson models should be interpreted differently from other regression models.
https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/poisson-regression",""
"Scenario: Executive Services, Inc. is a Headhunter agency whose CEO is Lenny Rollie. Their main office is in Manhattan, New York, and has been wildly successful in placing top-level executives at many key companies around the world. Lenny has approached you to assist ESI in their internal IT projects and has asked you to advise the IT team on the following matter.
The current project that you are consulting on has the following requirements.
Required:
• Create a custom role named DataScientist in the Azure Machine Learning workspace.
• Role members must not be able to delete the workspace.
• Role members must not be able to create, update, or delete compute resource in the workspace.
• Role members must not be able to add new users to the workspace.
• The custom role must enforce the restrictions specified by the IT Operations team.
Which of the following JSON code segments should the team use to fulfill the requirements?","multiple-choice","{
“Name“: “Data Scientist Custom“,
“IsCustom“: true,
“Description“: “Project Data Scientist role.“,
“Actions“: [
“Microsoft.MachineLearningServices/workspaces/*/delete“,
“Microsoft.MachineLearningServices/workspaces/write“,
“Microsoft.MachineLearningServices/workspaces/computes/*/write“,
“Microsoft.MachineLearningServices/workspaces/computes/*/delete“,
“Microsoft.Authorization/*/write“
],
“NotActions“: []
“AssignableScopes”: [
“/subscriptions//resourceGroups/ml-rg/providers/Microsoft.MachineLearningServices/workspaces/ml-ws“
]
}","","{
“Name“: “Data Scientist Custom“,
“IsCustom“: true,
“Description“: “Project Data Scientist role.“,
“Actions“: [“*“],
“NotActions“: [
“Microsoft.MachineLearningServices/workspaces/*/delete“,
“Microsoft.MachineLearningServices/workspaces/write“,
“Microsoft.MachineLearningServices/workspaces/computes/*/write“,
“Microsoft.MachineLearningServices/workspaces/computes/*/delete“,
“Microsoft.Authorization/*/write“
],
“AssignableScopes“: [
“/subscriptions//resourceGroups//providers/Microsoft.MachineLearningServices/workspaces/“
]
}","","{
“Name“: “Data Scientist Custom“,
“IsCustom“: true,
“Description“: “Project Data Scientist role.“,
“Actions“: [“*“],
“NotActions“: [],
“AssignableScopes”: [
“/subscriptions//resourceGroups/ml-rg/providers/Microsoft.MachineLearningServices/workspaces/ml-ws“
]
}","","{
“Name“: “Data Scientist Custom“,
“IsCustom“: true,
“Description“: “Project Data Scientist role.“,
“Actions“: [],
“NotActions“: [“*”],
“AssignableScopes”: [
“/subscriptions//resourceGroups/ml-rg/providers/Microsoft.MachineLearningServices/workspaces/ml-ws“
]
}","","","","","","2","Correct
The custom role can do everything in the workspace except for the following actions:
• It can‘t create or update a compute resource.
• It can‘t delete a compute resource.
• It can‘t add, delete, or alter role assignments.
• It can‘t delete the workspace.
To create a custom role, first construct a role definition JSON file that specifies the permission and scope for the role. The following example defines a custom role named “Data Scientist Custom“ scoped at a specific workspace level: data_scientist_custom_role.json :
{
“Name“: “Data Scientist Custom“,
“IsCustom“: true,
“Description“: “Can run experiment but can‘t create or delete compute.“,
“Actions“: [“*“],
“NotActions“: [
“Microsoft.MachineLearningServices/workspaces/*/delete“,
“Microsoft.MachineLearningServices/workspaces/write“,
“Microsoft.MachineLearningServices/workspaces/computes/*/write“,
“Microsoft.MachineLearningServices/workspaces/computes/*/delete“,
“Microsoft.Authorization/*/write“
],
“AssignableScopes“: [
“/subscriptions//resourceGroups//providers/Microsoft.MachineLearningServices/workspaces/“
]
}
Manage access to an Azure Machine Learning workspace
Azure role-based access control (Azure RBAC) is used to manage access to Azure resources, such as the ability to create new resources or use existing ones. Users in your Azure Active Directory (Azure AD) are assigned specific roles, which grant access to resources. Azure provides both built-in roles and the ability to create custom roles.
Default roles
An Azure Machine Learning workspace is an Azure resource. Like other Azure resources, when a new Azure Machine Learning workspace is created, it comes with three default roles. You can add users to the workspace and assign them to one of these built-in roles.
[IMAGE: word-image-43535-373.png]
Manage workspace access
If you‘re an owner of a workspace, you can add and remove roles for the workspace. You can also assign roles to users. Use the following links to discover how to manage access:
• Azure portal UI
• PowerShell
• Azure CLI
• REST API
• Azure Resource Manager templates
If you have installed the Azure Machine Learning CLI, you can use CLI commands to assign roles to users:
Azure CLI
az ml workspace share -w -g –role –user
The user field is the email address of an existing user in the instance of Azure Active Directory where the workspace parent subscription lives. Here is an example of how to use this command:
Azure CLI
az ml workspace share -w my_workspace -g my_resource_group –role Contributor –user jdoe@contoson.com [LINK]
Create custom role
If the built-in roles are insufficient, you can create custom roles. Custom roles might have read, write, delete, and compute resource permissions in that workspace. You can make the role available at a specific workspace level, a specific resource group level, or a specific subscription level.
To create a custom role, first construct a role definition JSON file that specifies the permission and scope for the role. The following example defines a custom role named “Data Scientist Custom“ scoped at a specific workspace level:
data_scientist_custom_role.json :
https://docs.microsoft.com/en-us/azure/machine-learning/how-to-assign-roles",""
"True or False: To evaluate a regression model, the best practice is to compare the predicted labels to the actual labels in the validation dataset held back during training.","multiple-choice","1","","","","","","","","","","","","1","Correct
To evaluate a regression model, you could simply compare the predicted labels to the actual labels in the validation dataset to held back during training, but this is an imprecise process and doesn‘t provide a simple metric that you can use to compare the performance of multiple models.
In this example, you‘re going to see the Auto Price Training pipeline.
Add an Evaluate Model
1. Open the Auto Price Training pipeline.
2. In the pane on the left, in the Model Scoring & Evaluation section, drag an Evaluate Model module to the canvas, under the Score Model module, and connect the output of the Score Model module to the Scored dataset (left) input of the Evaluate Model module.
3. Ensure your pipeline looks like this:
[IMAGE: word-image-43535-374.png]
4. Select Submit, and run the pipeline using the existing experiment named mslearn-auto-training.
5. Wait for the experiment run to complete.
6. When the experiment run has completed, select the Evaluate Model module and in the settings pane, on the Outputs + logs tab, under Data outputs in the Evaluation results section, use the Visualize icon to view the results. These include the following regression performance metrics:
• Mean Absolute Error (MAE): The average difference between predicted values and true values. This value is based on the same units as the label, in this case dollars. The lower this value is, the better the model is predicting.
• Root Mean Squared Error (RMSE): The square root of the mean squared difference between predicted and true values. The result is a metric based on the same unit as the label (dollars). When compared to the MAE (above), a larger difference indicates greater variance in the individual errors (for example, with some errors being very small, while others are large).
• Relative Squared Error (RSE): A relative metric between 0 and 1 based on the square of the differences between predicted and true values. The closer to 0 this metric is, the better the model is performing. Because this metric is relative, it can be used to compare models where the labels are in different units.
• Relative Absolute Error (RAE): A relative metric between 0 and 1 based on the absolute differences between predicted and true values. The closer to 0 this metric is, the better the model is performing. Like RSE, this metric can be used to compare models where the labels are in different units.
• Coefficient of Determination (R2): This metric is more commonly referred to as R-Squared, and summarizes how much of the variance between predicted and true values is explained by the model. The closer to 1 this value is, the better the model is performing.
7. Close the Evaluate Model result visualization window.
You can try a different regression algorithm and compare the results by connecting the same outputs from the Split Data module to a second Train model module (with a different algorithm) and a second Score Model module; and then connecting the outputs of both Score Model modules to the same Evaluate Model module for a side-by-side comparison.
When you‘ve identified a model with evaluation metrics that meet your needs, you can prepare to use that model with new data.
https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-auto-train-models",""
"Scenario: The Serpent Society is a business enterprise considered one of the best-organized, most successful sector coalitions in operation today. Seth Voelker is the founder of the Society and has come to you for assistance with their Microsoft Azure implementation. Today the discussion is about a multi-class image classification deep learning model that uses a set of labelled photographs. The team has created the following code to select hyperparameter values when training the model.
from azureml.train.hyperdrive import BayesianParameterSampling
param_sampling = BayesianParameterSampling ({
“learning_rate”: uniform(0.01, 0.1),
“batch_size”: choice(16, 32, 64, 128)}
)
The team has created a list of statements about the scenario. Some are correct, some are not.
Based on this information, Seth has asked you to indicate which of the following statements are true (Select all that apply)","multi-select","Hyperparameter combinations for the runs are selected based on how previous samples performed in the previous experiment run.","","The learning rate value 0.09 may be used during model training.","","Bayesian sampling supports early termination.","","You can define an early termination policy for this hyperparameter tuning run.","","","","","","1,2","Correct
The following are true statements:
• Hyperparameter combinations for the runs are selected based on how previous samples performed in the previous experiment run.
• The learning rate value 0.09 may be used during model training.
Hyperparameters are adjustable parameters you choose to train a model that govern the training process itself. Azure Machine Learning allows you to automate hyperparameter exploration in an efficient manner, saving you significant time and resources. You specify the range of hyperparameter values and a maximum number of training runs. The system then automatically launches multiple simultaneous runs with different parameter configurations and finds the configuration that results in the best performance, measured by the metric you choose. Poorly performing training runs are automatically early terminated, reducing wastage of compute resources. These resources are instead used to explore other hyperparameter configurations.
uniform(low, high) – Returns a value uniformly distributed between low and high.
https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters
BayesianParameterSampling Class
Bayesian sampling is based on the Bayesian optimization algorithm. It picks samples based on how previous samples did, so that new samples improve the primary metric.
Bayesian sampling only supports choice, uniform, and quniform distributions over the search space.
Bayesian sampling does not support early termination. When using Bayesian sampling, set early_termination_policy = None.
https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.bayesianparametersamplingview=azure-ml-py",""
"Scenario: Pym Tech is a U.S.-based Technology manufacturer headed by Hank Pym. Their headquarters is located at Treasure Island, San Francisco California and business is booming.
The expansion plans are underway which have presented several IT challenges which Hank has contracted you to advise his IT staff on.
At the moment, the topic is Deep Learning Virtual Machines for Windows.
The team is working on a new project and the following tools are available:
• Vowpal Wabbit
• PowerBI Desktop
• Azure Data Factory
• Microsoft Cognitive Toolkit
Hank has given the following requirements.
Required:
• Build deep neural network (DNN) models
• Perform interactive data exploration and visualization
Which of the available tools is best to use for performing interactive data exploration and visualization?","multiple-choice","Microsoft Cognitive Toolkit","","PowerBI Desktop","","Vowpal Wabbit","","Azure Data Factory","","","","","","2","Correct
The best tool listed for performing interactive data exploration and visualization is Power BI.
Power BI Desktop is a free application you install on your local computer that lets you connect to, transform, and visualize your data. With Power BI Desktop, you can connect to multiple different sources of data, and combine them (often called modelling) into a data model. This data model lets you build visuals, and collections of visuals you can share as reports, with other people inside your organization. Most users who work on business intelligence projects use Power BI Desktop to create reports, and then use the Power BI service to share their reports with others.
[IMAGE: word-image-43535-375.png]
The most common uses for Power BI Desktop are as follows:
• Connect to data
• Transform and clean that data, to create a data model
• Create visuals, such as charts or graphs, that provide visual representations of the data
• Create reports that are collections of visuals, on one or more report pages
• Share reports with others by using the Power BI service
People most often responsible for such tasks are often considered data analysts (sometimes referred to as analysts) or business intelligence professionals (often referred to as report creators). However, many people who don‘t consider themselves an analyst or a report creator use Power BI Desktop to create compelling reports, or to pull data from various sources and build data models, which they can share with their coworkers and organizations.
https://docs.microsoft.com/en-us/power-bi/fundamentals/desktop-what-is-desktop",""
"Scenario: The Dufours Precision Manufacturing Corporation was the business inherited by Countess Stephanie de la Spiroza from her late husband.
The Countess is creating a notebook on Azure Databricks to train her datasets, before using them with Spark ML.
Which of the following languages are supported for doing that in a notebook?","multiple-choice","C","","JavaScript","","Python","","C#","","Java","","","","3","Correct
Azure Databricks supports the following programming languages in runnable notebook cells: Python, Scala, SQL, and R.
Provision Azure Databricks workspaces and clusters
Two of the key concepts you need to be familiar with when working with Azure Databricks are workspaces and clusters.
Workspaces
A workspace is an environment for accessing all of your Databricks elements:
It groups objects (like notebooks, libraries, experiments) into folders,
Provides access to your data,
Provides access to the computations resources used (clusters, jobs).
[IMAGE: word-image-43535-376.png]
Each user has a home folder for their notebooks and libraries. The objects stored in the Workspace root folder are: folders, notebooks, libraries, and experiments.
To perform an action on a Workspace object, we can right-click the object and choose one of the available actions.
Clusters
A cluster is a set of computational resources on which you run your code (as notebooks or jobs). We can run ETL pipelines, or machine learning, data science, analytics workloads on the cluster.
We can create:
An all-purpose cluster. Multiple users can share such clusters to do collaborative interactive analysis.
A job cluster to run a specific job. The cluster will be terminated when the job completes (A job is a way of running a notebook or JAR either immediately or on a scheduled basis).
Before we can use a cluster, we have to choose one of the available runtimes.
Databricks runtimes are the set of core components that run on Azure Databricks clusters. Azure Databricks offers several types of runtimes:
Databricks Runtime: includes Apache Spark, components and updates that optimize the usability, performance, and security for big data analytics.
Databricks Runtime for Machine Learning: a variant that adds multiple machine learning libraries such as TensorFlow, Keras, and PyTorch.
Databricks Light: for jobs that don’t need the advanced performance, reliability, or autoscaling of the Databricks Runtime.
To create and configure a new cluster, we have to select the Create Cluster button and choose our options.
[IMAGE: word-image-43535-377.png]
We will see your new cluster appearing in the clusters list.
[IMAGE: word-image-43535-378.png]
To launch the cluster, we have to select the Start button and then confirm to launch it. It is recommended to wait until the cluster is started.
A cluster can be customized in many ways. In case you want to make third-party code available to your notebooks, you can install a library. Your cluster can be provisioned to use Python/Java/Scala/R libraries via PyPI or Maven.
Once the cluster is running, we can select Edit to change its properties. In case we want to provision your cluster with additional libraries, we can select the Libraries and then choose Install New.
[IMAGE: word-image-43535-379.png]
We can pick a library and it will be available later to be used in your notebooks.
https://learn.microsoft.com/en-us/azure/databricks/libraries/
Working with data in a workspace
An Azure Databricks database is a collection of tables. An Azure Databricks table is a collection of structured data.
We can cache, filter, and perform any operations supported by Apache Spark DataFrames on Azure Databricks tables. We can query tables with Spark APIs and Spark SQL.
To access our data:
We can import our files to DBFS using the UI.
We can mount and use supported data sources via DBFS.
We can then use Spark or local APIs to access the data.
We will be able to use a DBFS file path in our notebook to access our data, independent of its data source.
It is possible to import existing data or code in the workspace.
If we use small data files on the local machine that we want to analyze with Azure Databricks, we can import them to DBFS using the UI. There are two ways to upload data to DBFS with the UI:
Upload files to the FileStore in the Upload Data UI.
Upload data to a table with the Create table UI, which is also accessible via the Import & Explore Data box on the landing page.
We may also read data on cluster nodes using Spark APIs. We can read data imported to DBFS into Apache Spark DataFrames. For example, if you import a CSV file, you can read the data using this code
CSV
df = spark.read.csv(‘/FileStore/tables/nyc_taxi.csv‘, header=“true“, inferSchema=“true“)
We can also read data imported to DBFS in programs running on the Spark driver node using local file APIs. For example:
CSV
df = spark.read.csv(‘/dbfs/FileStore/tables/nyc_taxi.csv‘, header=“true“, inferSchema=“true“)
Importing data
To add data, we can go to the landing page and select Import & Explore Data.
To get the data in a table, there are multiple options available:
Upload a local file and import the data.
Use data already existing under DBFS.
Mount external data sources, like Azure Storage, Azure Data Lake and more.
To create a table based on a local file, we can select Upload File to upload data from your local machine.
[IMAGE: word-image-43535-380.png]
Once the data is uploaded, it will be available as a table or as a mountpoint under the DBFS filesystem (/FileStore).
Databricks can create a table automatically if we select Create Table with UI.
[IMAGE: word-image-43535-381.png]
Alternately, we can have full control over the structure of the new table by choosing Create Table in Notebook. Azure Databricks will generate Spark code that loads your data (and we can customize it via the Spark API).
[IMAGE: word-image-43535-382.png]
Using DBFS mounted data
Databricks File System (DBFS) is a distributed file system mounted into a Databricks workspace and available on Databricks clusters. DBFS is an abstraction on top of scalable object storage and offers the following benefits:
Allows you to mount storage objects so that you can seamlessly access data without requiring credentials.
Allows you to interact with object storage using directory and file semantics instead of storage URLs.
Persists files to object storage, so you won’t lose data after you terminate a cluster.
The default storage location in DBFS is known as the DBFS root.
We can use the DBFS to access:
Local files (previously imported). For example, the tables you imported above are available under /FileStore
Remote files, objects kept in separate storages as if they were on the local file system
For example, to mount a remote Azure storage account as a DBFS folder, we can use the dbutils module:
Python
data_storage_account_name = ‘‘
data_storage_account_key = ‘‘
data_mount_point = ‘/mnt/data‘
data_file_path = ‘/bronze/wwi-factsale.csv‘
dbutils.fs.mount(
source = f“wasbs://dev@{data_storage_account_name}.blob.core.windows.net“,
mount_point = data_mount_point,
extra_configs = {f“fs.azure.account.key.{data_storage_account_name}.blob.core.windows.net“: data_storage_account_key})
display(dbutils.fs.ls(“/mnt/data“))
#this path is available as dbfs:/mnt/data for spark APIs, e.g. spark.read
#this path is available as file:/dbfs/mnt/data for regular APIs, e.g. os.listdir
Notebooks support a shorthand – %fs magic command – for accessing the dbutils filesystem module. Most dbutils.fs commands are available using %fs magic commands:
BASH
# List the DBFS root
%fs ls
# Overwrite the file “/mnt/my-file“ with the string “Hello world!“
%fs put -f “/mnt/my-file“ “Hello world!“
https://learn.microsoft.com/en-us/azure/databricks/data/databricks-file-system",""
"Scenario: The Serpent Society is a business enterprise considered one of the best-organized, most successful sector coalitions in operation today. Seth Voelker is the founder of the Society and has come to you for assistance with their Microsoft Azure implementation. Today the discussion is about a comma-separated values (CSV) file containing data from which you want to train a classification model.
The team is using the Automated Machine Learning interface in Azure Machine Learning studio to train the classification model. The team leader set the task type to Classification.
The team needs to ensure that the Automated Machine Learning process evaluates only linear models.
Which of the following should the team do?","multiple-choice","Set the Exit criterion option to a metric score threshold.","","Set the task type to Regression.","","Clear the option to perform automatic featurization.","","Clear the option to enable deep learning.","","Add all algorithms other than linear ones to the blocked algorithms list.","","","","5","Correct
The correct action for the team is to add all algorithms other than linear ones to the blocked algorithms list.
Automated machine learning, AutoML, is a process in which the best machine learning algorithm to use for your specific data is selected for you. This process enables you to generate machine learning models quickly.
https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-automated-ml-for-ml-models",""
"A confusion matrix is an N X N matrix that is used to evaluate the performance of a classification model, where N is the number of target classes. It compares the actual target values against the ones predicted by the ML model. As a result, it provides a holistic view of how a classification model will work and the errors it will face.
[IMAGE: word-image-43535-383.png]
From these core values, you can calculate a range of other metrics that can help you evaluate the performance of the model.
Which of the following will tell us, of all the cases that the model predicted to be positive, how many actually are positive?","multiple-choice","2(TP/(TP+FP) * TP/(TP+FN))/(TP/(TP+FP) + TP/(TP+FN))","","(TP+TN)/(TP+TN+FP+FN)","","TP/(TP+FN)","","TP/(TP+FP)","","","","","","4","Correct
From these core values, you can calculate a range of other metrics that can help you evaluate the performance of the model. For example:
Accuracy: (TP+TN)/(TP+TN+FP+FN) – out all of the predictions, how many were correct?
Recall: TP/(TP+FN) – of all the cases that are positive, how many did the model identify?
Precision: TP/(TP+FP) – of all the cases that the model predicted to be positive, how many actually are positive?
What is a confusion matrix?
A confusion matrix is an N X N matrix that is used to evaluate the performance of a classification model, where N is the number of target classes. It compares the actual target values against the ones predicted by the ML model. As a result, it provides a holistic view of how a classification model will work and the errors it will face.
Running a classification model produces two outcomes:
Binary 0
Binary 1
where 0 means False and 1 means True.
Based on these two outcomes, we can easily compare the resulting classification outcomes with the actual values of the observation. This helps in judging the performance of a classification model. The matrix used in these outcomes is known as the confusion matrix.
The training accuracy of a classification model is much less important than how well that model will work when given new, unseen data. After all, we train models so that they can be used on new data we find in the real world. So, after we have trained a classification model, we should evaluate how it performs on a set of new, unseen data.
This sample data may be used to create a model that would predict whether a patient had diabetes or not based on their blood glucose level. Now, when applied to some data that wasn‘t part of the training set we get the following predictions:
[IMAGE: word-image-43535-384.png]
Here, x refers to blood glucose level, y refers to whether they‘re actually diabetic, and ŷ refers to the model’s prediction as to whether they‘re diabetic or not.
Simply calculating how many predictions were correct is sometimes misleading or too simplistic for us to understand the kinds of errors it will make in the real world. To get more detailed information, we can tabulate the results in a structure called a confusion matrix, like this:
[IMAGE: word-image-43535-385.png]
The confusion matrix shows the total number of cases where:
The model predicted 0 and the actual label is 0 (true negatives; top left)
The model predicted 1 and the actual label is 1 (true positives; bottom right)
The model predicted 0 and the actual label is 1 (false negatives; bottom left)
The model predicted 1 and the actual label is 0 (false positives; top right)
The cells in the confusion matrix are often shaded so that higher values have a deeper shade. This makes it easier to see a strong diagonal trend from top-left to bottom-right, highlighting the cells where the predicted value and actual value are the same.
From these core values, you can calculate a range of other metrics that can help you evaluate the performance of the model. For example:
Accuracy: (TP+TN)/(TP+TN+FP+FN) – out all of the predictions, how many were correct?
Recall: TP/(TP+FN) – of all the cases that are positive, how many did the model identify?
Precision: TP/(TP+FP) – of all the cases that the model predicted to be positive, how many actually are positivehttps://www.turing.com/kb/how-to-plot-confusion-matrix",""
"Scenario: Executive Services, Inc. is a Headhunter agency whose CEO is Lenny Rollie. Their main office is in Manhattan, New York, and has been wildly successful in placing top-level executives at many key companies around the world. Lenny has approached you to assist ESI in their internal IT projects and has asked you to advise the IT team on the following matter.
The current project that you are consulting on is using Azure Machine Learning designer to create a real-time service endpoint. The team has a single Azure Machine Learning service compute resource and they train the model and prepare the real-time pipeline for deployment.
Required: Publish the inference pipeline as a web service.
Which of the following compute types should the team use?","multiple-choice","Azure Kubernetes Services","","The existing Machine Learning Compute resource","","A new Machine Learning Compute resource","","HDInsight","","Azure Databricks","","","","1","Correct
The team should use Azure Kubernetes Services because of the listed options, only AKS can be used for real-time interference.
Compute targets for inference
When performing inference, Azure Machine Learning creates a Docker container that hosts the model and associated resources needed to use it. This container is then used in a compute target.
[IMAGE: word-image-43535-386.png]
The compute target you use to host your model will affect the cost and availability of your deployed endpoint. Use this table to choose an appropriate compute target.
https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target",""
"Scenario: The UCWF (Unlimited Class Wrestling Federation) was founded by a promoter named Edward Garner, who strives to use technology to improve his business and has come to you for assistance with the company’s Microsoft Azure service.
At this moment, the team is using Azure Machine Learning Designer to build a machine learning experiment and needs to divide data into two distinct datasets.
Which of the following modules should the team use?","multiple-choice","Group Data into Bins","","Assign Data to Clusters","","Load Trained Model","","Split Data","","","","","","4","Correct
The team should use the Split Date module to divide data into two distinct datasets.
Split Data
Partitions the rows of a dataset into two distinct sets
Category: Data Transformation / Sample and Split
Note: Applies to: Machine Learning Studio (classic)
This content pertains only to Studio (classic). Similar drag and drop modules have been added to Azure Machine Learning Designer. Learn more in this article comparing the two versions.
Module overview
This topic describes how to use the Split Data module in Azure Machine Learning Studio (classic), to divide a dataset into two distinct sets.
This module is particularly useful when you need to separate data into training and testing sets. You can customize the way that data is divided as well. Some options support randomization of data; others are tailored for a certain data type or model type.
How to configure Split Data
1. Add the Split Data module to your experiment in Designer. You can find this module under Data Transformation, in the Sample and Split category.
2. Splitting mode: Choose one of the following modes, depending on the type of data you have, and how you want to divide it. Each splitting mode has different options. Click the following topics for detailed instructions and examples.
1. Split Rows: Use this option if you just want to divide the data into two parts. You can specify the percentage of data to put in each split, but by default, the data is divided 50-50.
2. You can also randomize the selection of rows in each group, and use stratified sampling. In stratified sampling, you must select a single column of data for which you want values to be apportioned equally among the two result datasets.
3. Recommender Split: Always choose this option if you are preparing data for use in a recommender system. It helps you divide data sets into training and testing groups while ensuring that important values such as user-item pairs or ratings are evenly divided among the groups.
4. Regular Expression Split: Choose this option when you want to divide your dataset by testing a single column for a value.
5. For example, if you are analyzing sentiment, you could check for the presence of a particular product name in a text field, and then divide the dataset into rows with the target product name, and those without.
6. Relative Expression Split: Use this option whenever you want to apply a condition to a number column. The number could be a date/time field, a column containing age or dollar amounts, or even a percentage. For example, you might want to divide your data set depending on the cost of the items, group people by age ranges, or separate data by a calendar date.
https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/split-data",""
"Scenario: The Serpent Society is a business enterprise considered one of the best-organized, most successful sector coalitions in operation today. Seth Voelker is the founder of the Society and has come to you for assistance with their Microsoft Azure implementation. Today the discussion is about analyzing the asymmetry in a statistical distribution.
The following image contains two density curves that show the probability distribution of two datasets.
[IMAGE: word-image-43535-387.png]
The team is analyzing the curves to determine the distribution type of the dataset density curves.
What distribution type is Graph 2?","multiple-choice","Positive skew","","Normal distribution","","Negative skew","","Bimodal distribution","","","","","","1","Correct
Graph 1 is negative skew and Graph 2 is positive skew.
A left-skewed distribution has a long left tail. Left-skewed distributions are also called negatively-skewed distributions. That’s because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.
A right-skewed distribution has a long right tail. Right-skewed distributions are also called positive-skew distributions. That’s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak. https://www.statisticshowto.com/probability-and-statistics/skewed-distribution/",""
"Scenario: The UCWF (Unlimited Class Wrestling Federation) was founded by a promoter named Edward Garner, who strives to use technology to improve his business and has come to you for assistance with the company’s Microsoft Azure service.
At this moment, the IT team is planning to deliver a hands-on workshop to several interns. The workshop will focus on creating data visualizations using Python. Each intern will use a device that has internet access.
Intern devices are not configured for Python development and interns do not have administrator access to install software on their devices. Azure subscriptions are not available for students.
The facilitator of the workshops needs to ensure that students can run Python-based data visualization code.
Which tool should they use?","multiple-choice","Azure Machine Learning Service","","GitHub Codespaces","","Anaconda Data Science Platform","","Azure Notebooks","","Azure BatchAl","","","","2","Correct
Azure notebooks was the correct answer, but it is now retired. Now, you should use GitHub Codespaces.
GitHub Codespaces
GitHub Codespaces supports Visual Designer Code and modern web browsers. With your development in the cloud, seamlessly switch between tools and contribute code from anywhere, anytime.
[IMAGE: word-image-43535-388.jpeg]
https://github.githubassets.com/images/modules/site/codespaces/hero.h264.mp4
https://visualstudio.microsoft.com/services/github-codespaces/",""
"You are creating a pipeline that includes a step to train a model using an estimator.
Which kind of step should you define in the pipeline for this task.","multiple-choice","EstimatorStep","","DatabricksStep","","PythonScriptStep","","","","","","","","1","Correct
Use an EstimatorStep to run an estimator in a pipeline.",""
"You plan to use hyperparameter tuning to find optimal discrete values for a set of hyperparameters.
You want to try every possible combination of a set of specified discrete values.
Which kind of sampling should you use?","multiple-choice","Random Sampling","","Bayesian Sampling","","Grid Sampling","","","","","","","","3","Correct
You should use a Grid sampling to try every combination of discrete hyperparameter values.",""
"You have trained a model, and you want to quantify the influence of each feature on a specific individual prediction.
What kind of feature importance should you examine?","multiple-choice","Local feature importance","","Global feature importance","","","","","","","","","","1","Correct
Local importance indicates the influence of features on a specific prediction. Global importance gives an overall indication of feature influence.",""
"You want to include model explanations in the logged details of your training experiment.
What must you do in your training script?","multiple-choice","Use the ExplanationClient.upload_model_explanation method to upload the explanation created by an Explainer.","","Save the the explanation created by an Explainer in the ./outputs folder.","","Use the Run.log_table method to log feature importance for each feature.","","","","","","","","1","Correct
To include an explanation in the run details, the training script must use the ExplanationClient.upload_model_explanation method to upload the explanation created by an Explainer.",""
"You are deploying a model as a real-time inferencing service. What functions must the entry script for the service include?","multiple-choice","load() and score(raw_data)","","main() and predict(raw_data)","","init() and run(raw_data)","","","","","","","","3","Correct
You need to implement init and run functions in the entry (scoring) script.",""
"You have registered a dataset in your workspace.
You want to use the dataset in an experiment script that is run using an estimator.
What should you do?","multiple-choice","Use the dataset to save the data as a CSV file in the experiment script folder before running the experiment.","","Pass the dataset as a named input to the estimator.","","Create a data reference for the datastore location where the dataset data is stored, and pass it to the script as a parameter.","","","","","","","","2","Correct
To access a dataset in an experiment script, pass the dataset as a named input to the estimator.",""
"You have published a pipeline that you want to run every week.
You plan to use the Schedule.create method to create the schedule.
What kind of object must you create first to configure how frequently the pipeline runs?","multiple-choice","PipelineParameter","","Datastore","","ScheduleRecurrance","","","","","","","","3","Correct
You need a ScheduleRecurrance object to create a schedule that runs at a regular interval.",""
"You need a cloud-based development environment that you can use to run Jupyter notebooks that are stored in your workspace. The notebooks must remain in your workspace at all times.
What should you do?","multiple-choice","Create a Compute Instance compute target in your workspace.","","Create a Training Cluster compute target in your workspace.","","Install Visual Studio Code on your local computer.","","","","","","","","1","Correct
Compute Instances provide a cloud-based development environment that supports Jupyter Notebooks in your workspace. You can save notebooks in the workspace and work on them there.",""
"Which edition of Azure Machine Learning workspace should you provision if you only plan to use the graphical Designer tool to train machine learning models?","multiple-choice","Basic","","Enterprise","","","","","","","","","","2","Correct
The visual Designer tool is not available in Basic edition workspaces, so you must create an Enterprise workspace to use it.",""
"You have used the Python SDK for Azure Machine Learning to create a pipeline that trains a model.
What do you need to do so that a client application can invoke the pipeline through an HTTP REST endpoint?","multiple-choice","Rename the pipeline to pipeline_name-production.","","Create an inference cluster compute target.","","Publish the pipeline.","","","","","","","","3","Correct
You must publish a pipeline to create an endpoint.",""
"You want to run a script as an experiment.
You have already created a RunConfig object to define the Python runtime context for the experiment.
What other object should you create to associate the script with the runtime context?","multiple-choice","A Pipeline object.","","A ScriptRunConfig object.","","A ComputeTarget object.","","","","","","","","2","Correct
To associate a script with a RunConfig, you must use a ScriptRunConfig object.",""
"You are creating a batch inferencing pipeline that you want to use to predict new values for a large volume of data files?
You want the pipeline to run the scoring script on multiple nodes and collate the results.
What kind of step should you include in the pipeline?","multiple-choice","ParallelRunStep","","AdlaStep","","PythonScriptStep","","","","","","","","1","Correct
You should use a ParallelRunStep step to run the scoring script in parallel.",""
"You plan to use the Workspace.from_config() method to connect to your Azure Machine Learning workspace from a Python environment on your local workstation. You have already used pip to install the azureml-sdk package.
What else should you do?","multiple-choice","Run pip install azureml-sdk[‘notebooks‘] to install the notebooks extra.","","Download the config.json file for your workspace to the folder containing your local Python code files.","","Create a Compute Instance compute target in your workspace.","","","","","","","","2","Correct
To connect to your Azure Machine Learning workspace using the Workspace.from_config() method in a Python environment, you have already installed the azureml-sdk package.
Here’s what else you should do:
Options Analysis
A. Run pip install azureml-sdk[‘notebooks‘] to install the notebooks extra.
This option is useful if you plan to work with Jupyter notebooks that utilize the Azure Machine Learning SDK, but it is not strictly necessary for using Workspace.from_config(). It installs additional components related to notebooks but does not directly affect the ability to connect to the workspace.
B. Download the config.json file for your workspace to the folder containing your local Python code files.
This is essential for using Workspace.from_config(). The config.json file contains the necessary configuration details (like subscription ID, resource group, and workspace name) required to establish a connection to your Azure Machine Learning workspace. Without this file, the method cannot function properly.
C. Create a Compute Instance compute target in your workspace.
While creating a Compute Instance may be necessary for running experiments or training models, it is not required for connecting to the workspace using Workspace.from_config(). You can connect to the workspace without having any compute resources set up.
Conclusion
The most critical step you should take after installing the azureml-sdk package is: B. Download the config.json file for your workspace to the folder containing your local Python code files. This step is necessary for the Workspace.from_config() method to work correctly. Therefore, the correct answer is B.",""
"You are using automated machine learning, and you want to determine the influence of features on the predictions made by the best model produced by the automated machine learning experiment.
What must you do when configuring the automated machine learning experiment?","multiple-choice","Whitelist only tree-based algorithms.","","Enable featurization.","","Enable model explainability.","","","","","","","","3","Correct
To generate model explanations when using automated machine learning, you must enable model explainability.",""
"You have trained a model using the Python SDK for Azure Machine Learning.
You want to deploy the model as a containerized real-time service with high scalability and security.
What kind of compute should you create to host the service?","multiple-choice","A compute instance with GPUs.","","A training cluster with multiple nodes.","","An Azure Kubernetes Services (AKS) inferencing cluster.","","","","","","","","3","Correct
You should use an AKS cluster to deploy a model as a scalable, secure, containerized service.",""
"You want to include custom information in the telemetry for your inferencing service, and analyze it using Application Insights.
What must you do in your service‘s entry script?","multiple-choice","Save the custom metrics in the ./outputs folder.","","Use the Run.log method to log the custom metrics.","","Use a print statement to write the metrics in the STDOUT log.","","","","","","","","3","Correct
To include custom metrics, add print statements to the scoring script so that the custom information is written to the STDOUT log.",""
"You have submitted an automated machine learning run using the Python SDk for Azure Machine Learning.
When the run completes, which method of the run object should you use to retrieve the best model?","multiple-choice","load_model()","","get_output()","","get_metrics()","","","","","","","","2","Correct
The get_output method of an automated machine learning run returns the best mode and the child run that trained it.",""
"You have written a script that uses the Scikit-Learn framework to train a model.
Which framework-specific estimator should you use to run the script as an experiment?","multiple-choice","Tensorflow","","PyTorch","","SKLearn","","","","","","","","3","Correct
To run a scikit-learn training script as an experiment, use the generic Estimator estimator or a SKLearn estimator.",""
"You are using the Azure Machine Learning Python SDK to write code for an experiment. You need to record metrics from each run of the experiment, and be able to retrieve them easily from each run.
What should you do?","multiple-choice","Save the experiment data in the outputs folder.","","Use the log methods of the Run class to record named metrics.","","Add print statements to the experiment code to print the metrics.","","","","","","","","2","Correct
To record metrics in an experiment run, use the Run.log methods.",""
"You have configured the step in your batch inferencing pipeline with an output_action=“append_row“ property.
In which file should you look for the batch inferencing results?","multiple-choice","parallel_run_step.txt","","stdoutlogs.txt","","output.txt","","","","","","","","1","Correct
Using the append_row output action causes the results from the ParallelRunStep step to be collated in a file named parallel_run_step.txt.",""
"You have created a pipeline that includes multiple modules to define a dataflow and train a model.
Now you want to run the pipeline.
What must you do first?","multiple-choice","Create a Training Cluster in your workspace, and select it as the compute target for the pipeline.","","Add comments to each of the modules on the pipeline canvas.","","Rename the pipeline to include the date and time.","","","","","","","","1","Correct
To run a pipeline, you need a Training Cluster compute target in the workspace.",""
"You are using automated machine learning to train a model that predicts the species of an iris based on its petal and sepal measurements.
Which kind of task should you specify for automated machine learning?","multiple-choice","Forecasting","","Regression","","Classification","","","","","","","","3","Correct
Predicting a class requires a classification task.",""
"You are creating a pipeline that includes two steps.
Step 1 preprocesses some data, and step 2 uses the preprocessed data to train a model.
What type of object should you use to pass data from step 1 to step 2 and create a dependency between these steps?","multiple-choice","Datastore","","Data Reference","","PipelineData","","","","","","","","3","Correct
To pass data between steps in a pipeline, use a PipelineData object.",""
"You have deployed a model as a real-time inferencing service in an Azure Kubernetes Service (AKS) cluster.
What must you do to capture and analyze telemetry for this service?","multiple-choice","Move the AKS cluster to the same region as the Azure Machine Learning workspace.","","Implement inference-time model interpretability.","","Enable application insights.","","","","","","","","3","Correct
To enable telemetry analysis though Application Insghts, you must enable Application Insights for the service.",""
"You have run an experiment to train a model.
You want the model to be stored in the workspace, and available to other experiments and published services.
What should you do?","multiple-choice","Register the model in the workspace.","","Save the experiment script as a notebook.","","Save the model as a file in a Compute Instance.","","","","","","","","1","Correct
To store a model in the workspace, register it.",""
"Which of the following descriptions accurately describes Azure Machine Learning?","multiple-choice","An application for Microsoft Windows that enables you to create machine learning models by using a drag and drop interface.","","A Python library that you can use as an alternative to common machine learning frameworks like Scikit-Learn, PyTorch, and Tensorflow.","","A cloud-based platform for operating machine learning solutions at scale.","","","","","","","","3","Correct
Azure Machine Learning is an Azure service that you can use to manage machine learning model data preparation, training, validation, and deployment. It leverages existing frameworks such as Scikit-Learn, PyTorch, and Tensorflow; and provides a cross-platform platform for operationalizing machine learning in the cloud.",""
"You are solving a classification task.
The dataset is imbalanced.
You need to select an Azure Machine Learning Studio module to improve the classification accuracy.
Which module should you use?","multiple-choice","Fisher Linear Discriminant Analysis","","Synthetic Minority Oversampling Technique (SMOTE)","","Permutation Feature Importance","","Filter Based Feature Selection","","","","","","2","Correct
Use the SMOTE module in Azure Machine Learning Studio (classic) to increase the number of underepresented cases in a dataset used for machine learning. SMOTE is a better way of increasing the number of rare cases than simply duplicating existing cases.
You connect the SMOTE module to a dataset that is imbalanced. There are many reasons why a dataset might be imbalanced: the category you are targeting might be very rare in the population, or the data might simply be difficult to collect. Typically, you use SMOTE when the class you want to analyze is under-represented.
Reference:
https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/smote",""
"You have trained a model using a dataset containing data that was collected last year. As this year progresses, you will collect new data.
You want to track any changing data trends that might affect the performance of the model.
What should you do?","multiple-choice","Collect the new data in a new version of the existing training dataset, and profile both datasets.","","Collect the new data in a separate dataset and create a Data Drift Monitor with the training dataset as a baseline and the new dataset as a target.","","Replace the training dataset with a new dataset that contains both the original training data and the new data.","","","","","","","","2","Correct
To track changing data trends, create a data drift monitor that uses the training data as a baseline and the new data as a target.",""
"You want to create an explainer that applies the most appropriate SHAP model explanation algorithm based on the type of model.
What kind of explainer should you create?","multiple-choice","Permutation Feature Importance","","Tabular","","Mimic","","","","","","","","2","Correct
A Tabular explainer applies the most appropriate SHAP model interpretation algorithm for the type of model.",""
"You have uploaded some data files to a folder in a blob container, and registered the blob container as a datastore in your Azure Machine Learning workspace.
You want to run a script as an experiment that loads the data files and trains a model.
What should you do?","multiple-choice","Create global variables for the Azure Storage account name and key in the experiment script.","","Create a data reference for the datastore location and pass it to the script as a parameter.","","Save the experiment script in the same blob folder as the data files.","","","","","","","","2","Correct
To access a path in a datastore in an experiment script, you must create a data reference and pass it to the script as a parameter. The script can then read data from the data reference parameter just like a local file path.",""
"You are creating a data drift monitor.
You want to automatically notify the data science team if a significant change in data distribution is detected.
What must you do?","multiple-choice","Define an AlertConfiguration and set a drift_threshold value.","","Set the latency of the data drift monitor to allow time for data scientists to review the new data.","","Register the training dataset with the model, including the email address of the data science team as a tag.","","","","","","","","1","Correct
To notify operators about data drift, create an AlertConfiguration with the email address to be notified, and a drift threshold that defines the level of change that triggers a notification.",""
"You have published a pipeline as a real-time service on an Azure Kubernetes Services (AKS) cluster.
An application developer plans to call the service from a REST-based client.
What information does the application developer require?","multiple-choice","The name of the AKS compute target in the workspace.","","The endpoint URL and key for the published service.","","The name of the inference pipeline in designer.","","","","","","","","2","Correct
To make a REST call to a published service, you need the service endpoint URL and authorization key.",""
"You are using an estimator to run an experiment, and you want to run it on a compute cluster named training-cluster-1. Which property of the estimator should you set to run the experiment on training-cluster-1?","multiple-choice","source_directory = ‘training-cluster-1‘","","environment_definition = ‘training-cluster-1‘","","compute_target = ‘training-cluster-1‘","","","","","","","","3","Correct
To specify an compute target for an estimator, use the compute_target parameter.",""
"You have a CSV file containing structured data that you want to use to train a model.
You upload the file to a folder in an Azure Storage blob container, for which a datastore is defined in your workspace. Now you want to create a dataset for the data so that it can be easily used as a Pandas dataframe.
Which kind of dataset should you create?","multiple-choice","A file dataset","","A tabular dataset","","","","","","","","","","2","Correct
Use a tabular dataset for structured data that you want to work with in a Pandas dataframe.",""
"You need to create a compute target for training experiments that require a graphical processing unit (GPU).
You want to be able to scale the compute so that multiple nodes are started automatically as required.
Which kind of compute target should you create.","multiple-choice","Inference Cluster","","Compute Cluster","","Compute Instance","","","","","","","","2","Correct
Use a compute cluster to create multiple nodes of GPU-enabled VMs that are started automatically as needed.",""
"You have created and run a pipeline to train a model using the Designer tool. Now you want to publish it as a real-time service.
What must you do first?","multiple-choice","Change the compute target of the training pipeline to an Azure Kubernetes Services (AKS) cluster.","","Create an inference pipeline from your training pipeline.","","Clone the training pipeline with a different name.","","","","","","","","2","Correct
Before you can publish a pipeline as a service, you must create an inference pipeline from the training pipeline, and modify the web service inputs, outputs, and data flow as necessary for production inferencing.",""
"You plan to use the Workspace.from_config() method to connect to your Azure Machine Learning workspace from a Python environment on your local workstation. You have already used pip to install the azureml-sdk package.
What else should you do?","multiple-choice","Create a Compute Instance compute target in your workspace.","","Download the config.json file for your workspace to the folder containing your local Python code files.","","What else should you do? Run pip install azureml-sdk[‘notebooks‘] to install the notebooks extra.","","","","","","","","2","Correct
B. Download the config.json file for your workspace to the folder containing your local Python code files.
Explanation:
Workspace.from_config() method specifically looks for a config.json file in the current working directory to establish a connection to your Azure Machine Learning workspace.
This file contains essential authentication and workspace information.
Therefore, downloading the config.json file is the crucial step to enable the Workspace.from_config() method to work correctly.
The other options are not directly related to using the Workspace.from_config() method.",""
"You need to ingest data from a CSV file into a pipeline in Designer. What should you do?","multiple-choice","Create a Dataset by uploading the file, and drag the dataset to the canvas.","","Add a Convert to CSV module to the canvas.","","Add an Enter Data Manually module to the canvas.","","","","","","","","1","Correct
The recommended way to ingest data is to create a dataset and drag it to the canvas.",""
"You plan to build a team data science environment. Data for training models in machine learning pipelines will be over 20 GB in size.
You have the following requirements:
Models must be built using Caffe2 or Chainer frameworks.
Data scientists must be able to use a data science environment to build the machine learning pipelines and train models on their personal devices in both connected and disconnected network environments.
Personal devices must support updating machine learning pipelines when connected to a network.
You need to select a data science environment.
Which environment should you use?","multiple-choice","Azure Kubernetes Service (AKS)","","Azure Machine Learning Studio","","Azure Machine Learning Service","","Azure Databricks","","","","","","3","Correct
The Data Science Virtual Machine (DSVM) is a customized VM image on Microsoft’s Azure cloud built specifically for doing data science. Caffe2 and Chainer are supported by DSVM. DSVM integrates with Azure Machine Learning.
References:
https://docs.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/overview",""
"You are implementing a machine learning model to predict stock prices.
The model uses a PostgreSQL database and requires GPU processing.\
You need to create a virtual machine that is pre-configured with the required tools.
What should you do?","multiple-choice","Create a Deep Learning Virtual Machine (DLVM) Windows edition.","","Create a Geo Al Data Science Virtual Machine (Geo-DSVM) Windows edition.","","Create a Deep Learning Virtual Machine (DLVM) Linux edition.","","Create a Data Science Virtual Machine (DSVM) Linux edition.","","Create a Data Science Virtual Machine (DSVM) Windows edition.","","","","4","Correct
Incorrect Answers:
A and C: PostgreSQL (CentOS) is only available in the Linux Edition.
B: The Azure Geo AI Data Science VM (Geo-DSVM) delivers geospatial analytics capabilities from Microsoft‘s Data Science VM. Specifically, this VM extends the AI and data science toolkits in the Data Science VM by adding ESRI‘s market-leading ArcGIS Pro Geographic Information System.
D: DLVM is a template on top of DSVM image. In terms of the packages, GPU drivers etc are all there in the DSVM image. Mostly it is for convenience during creation where we only allow DLVM to be created on GPU VM instances on Azure.
References:
https://docs.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/overview",""
"Your team is building a data engineering and data science development environment.
The environment must support the following requirements:
support Python and Scala
compose data storage, movement, and processing services into automated data pipelines
the same tool should be used for the orchestration of both data engineering and data science
support workload isolation and interactive workloads
enable scaling across a cluster of machines
You need to create the environment.
What should you do?","multiple-choice","Build the environment in Azure Databricks and use Azure Container Instances for orchestration.","","Build the environment in Apache Hive for HDInsight and use Azure Data Factory for orchestration.","","Build the environment in Azure Databricks and use Azure Data Factory for orchestration.","","Build the environment in Apache Spark for HDInsight and use Azure Container Instances for orchestration.","","","","","","3","Correct
In Azure Databricks, we can create two different types of clusters.
Standard, these are the default clusters and can be used with Python, R, Scala and SQL High-concurrency
Azure Databricks is fully integrated with Azure Data Factory.
Incorrect Answer:
Build the environment in Azure Databricks and use Azure Container Instances for orchestration.; Azure Container Instances is good for development or testing. Not suitable for production workloads.
References:
https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/data-science-and-machine-learning",""
"You are a data scientist in a company that provides data science for professional sporting events. Models will use global and local market data to meet the following business goals:
Understand sentiment of mobile device users at sporting events based on audio from crowd reactions.
Assess a user’s tendency to respond to an advertisement.
Customize styles of ads served on mobile devices.
Use video to detect penalty events
Current environment
Media used for penalty event detection will be provided by consumer devices. Media may include images and videos captured during the sporting event and shared using social media. The images and videos will have varying sizes and formats.
The data available for model building comprises of seven years of sporting event media. The sporting event media includes; recorded video transcripts or radio commentary, and logs from related social media feeds captured during the sporting events.
Crowd sentiment will include audio recordings submitted by event attendees in both mono and stereo formats.
Penalty detection and sentiment
Data scientists must build an intelligent solution by using multiple machine learning models for penalty event detection.
Data scientists must build notebooks in a local environment using automatic feature engineering and model building in machine learning pipelines.
Notebooks must be deployed to retrain by using Spark instances with dynamic worker allocation. Notebooks must execute with the same code on new Spark instances to recode only the source of the data.
Global penalty detection models must be trained by using dynamic runtime graph computation during training.
Local penalty detection models must be written by using BrainScript.
Experiments for local crowd sentiment models must combine local penalty detection data.
Crowd sentiment models must identify known sounds such as cheers and known catch phrases. Individual crowd sentiment models will detect similar sounds.
All shared features for local models are continuous variables.
Shared features must use double precision. Subsequent layers must have aggregate running mean and standard deviation metrics available.
Advertisements
During the initial weeks in production, the following was observed:
Ad response rated declined. Drops were not consistent across ad styles. The distribution of features across training and production data are not consistent
Analysis shows that, of the 100 numeric features on user location and behavior, the 47 features that come from location sources are being used as raw features. A suggested experiment to remedy the bias and variance issue is to engineer 10 linearly uncorrelated features.
Initial data discovery shows a wide range of densities of target states in training data used for crowd sentiment models.
All penalty detection models show inference phases using a Stochastic Gradient Descent (SGD) are running too slow.
Audio samples show that the length of a catch phrase varies between 25%-47% depending on region The performance of the global penalty detection models shows lower variance but higher bias when comparing training and validation sets. Before implementing any feature changes, you must confirm the bias and variance using all training and validation cases.
Ad response models must be trained at the beginning of each event and applied during the sporting event.
Market segmentation models must optimize for similar ad response history.
Sampling must guarantee mutual and collective exclusively between local and global segmentation models that share the same features.
Local market segmentation models will be applied before determining a user’s propensity to respond to an advertisement.
Ad response models must support non-linear boundaries of features.
The ad propensity model uses a cut threshold is 0.45 and retrains occur if weighted Kappa deviated from 0.1 +/- 5%.
The ad propensity model uses cost factors shown in the following diagram:
[IMAGE: word-image-42897-1.png]
The ad propensity model uses proposed cost factors shown in the following diagram:
[IMAGE: word-image-42897-2.png]
Performance curves of current and proposed cost factor scenarios are shown in the following diagram:
[IMAGE: word-image-42897-3.png]
You need to implement a new cost factor scenario for the ad response models as illustrated in the performance curve exhibit.
Which technique should you use?","multiple-choice","Set the threshold to 0.5 and retrain if weighted Kappa deviates +/- 5% from 0.45.","","Set the threshold to 0.2 and retrain if weighted Kappa deviates +/- 5% from 0.6.","","Set the threshold to 0.05 and retrain if weighted Kappa deviates +/- 5% from 0.5.","","Set the threshold to 0.75 and retrain if weighted Kappa deviates +/- 5% from 0.15.","","","","","","1","Correct
Performance curves of current and proposed cost factor scenarios are shown in the following diagram:
[IMAGE: word-image-42897-4.png]
The ad propensity model uses a cut threshold is 0.45 and retrains occur if weighted Kappa deviated from 0.1 +/- 5%.",""
"Scenario: Pennyworth‘s Haberdashery has been around for over 50 years now and they have several stores in the Greater London area. They have just purchased a smaller clothier line based in Madrid and are integrating their systems with Pennyworth‘s Microsoft Azure service.
Alfred Pennyworth is an avid birder and donates some of his company’s resources to a project that tracks the health and migration of birds.
The lead data scientist is creating a multi-class image classification deep learning model that uses a set of labelled bird photographs collected by experts.
So far, the project has collected 100,000 photographs of birds. All photographs use the JPG format and are stored in an Azure blob container in an Azure subscription.
The data scientist needs to access the bird photograph files in the Azure blob container from the Azure Machine Learning service workspace that will be used for deep learning model training and must minimize data movement.
Which of the following actions should the data scientist perform?","multiple-choice","Create and register a dataset by using TabularDataset class that references the Azure blob storage containing bird photographs.","","Create an Azure Cosmos DB database and attach the Azure Blob containing bird photographs storage to the database.","","Create an Azure Data Lake store and move the bird photographs to the store.","","Copy the bird photographs to the blob datastore that was created with your Azure Machine Learning service workspace.","","Register the Azure blob storage containing the bird photographs as a datastore in Azure Machine Learning service.","","","","5","Correct
The data scientist should register the Azure blob storage containing the bird photographs as a datastore in Azure Machine Learning service. When you create a workspace, an Azure blob container and an Azure file share are automatically registered to the workspace.
Connect to storage services on Azure
Datastores securely connect to your storage service on Azure without putting your authentication credentials and the integrity of your original data source at risk. They store connection information, like your subscription ID and token authorization in your Key Vault that‘s associated with the workspace, so you can securely access your storage without having to hard code them in your scripts. You can create datastores that connect to these Azure storage solutions.
For a low code experience, see how to use the Azure Machine Learning Designer to create and register datastores.
Either create an Azure Machine Learning workspace or use an existing one via the Python SDK.
Import the Workspace and Datastore class, and load your subscription information from the file config.json using the function from_config(). This looks for the JSON file in the current directory by default, but you can also specify a path parameter to point to the file using from_config(path=“your/file/path“).
Python
import azureml.core
from azureml.core import Workspace, Datastore
ws = Workspace.from_config()
When you create a workspace, an Azure blob container and an Azure file share are automatically registered as datastores to the workspace. They‘re named workspaceblobstore and workspacefilestore, respectively. The workspaceblobstore is used to store workspace artifacts and your machine learning experiment logs. It‘s also set as the default datastore and can‘t be deleted from the workspace. The workspacefilestore is used to store notebooks and R scripts authorized via compute instance.
Note: Azure Machine Learning designer will create a datastore named azureml_globaldatasets automatically when you open a sample in the designer homepage. This datastore only contains sample datasets. Please do not use this datastore for any confidential data access.
https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data",""
"Scenario: Beyond Corporation is an organization which originated on the planet Vulcan who is lead by S‘chn T‘gai Spock as the Supreme Commander. Spock has investigated the Microsoft platform and found it to be formidable and has decided to implement its use into Beyond.
The development team is using a Python script to train a model created in Azure Databricks which is attached to a cluster with Databricks Runtime for Machine Learning.
To run the code and enable automated MLflow when tuning hyperparameters, which method should be used?","multiple-choice","WebserviceDeploymentAutoMLflow","","RegressionEvaluator()","","CrossValidator","","ParamGridBuilder()","","","","","","3","Correct
CrossValidator or TrainValidationSplit can be used to tune hyperparameters with automated MLflow.
Automated MLflow for model tuning
To choose the best model trained during hyperparameter tuning, you want to compare all models by evaluating their metrics. One common and simple approach to track model training in Azure Databricks is by using the open-source platform MLflow.
Use automated MLflow
As you train multiple models with hyperparameter tuning, you want to avoid the need to make explicit API calls to log all necessary information about the different models to MLflow. To make tracking hyperparameter tuning easier, the Databricks Runtime for Machine Learning also supports automated MLflow Tracking. When you use automated MLflow for model tuning, the hyperparameter values and evaluation metrics are automatically logged in MLflow and a hierarchy will be created for the different runs that represent the distinct models you train.
To use automated MLflow tracking, you have to do the following:
Use a Python notebook to host your code.
Attach the notebook to a cluster with Databricks Runtime or Databricks Runtime for Machine Learning.
Set up the hyperparameter tuning with CrossValidator or TrainValidationSplit.
MLflow will automatically create a main or parent run that contains the information for the method you chose: CrossValidator or TrainValidationSplit. MLflow will also create child runs that are nested under the main or parent run. Each child run will represent a trained model and you can see which hyperparameter values were used and the resulting evaluation metrics.
Run tuning code
When you want to run code that will train multiple models with different hyperparameter settings, you can go through the following steps:
List the available hyperparameters for a specific algorithm.
Set up the search space and sampling method.
Run the code with automated MLflow, using CrossValidator or TrainValidationSplit.
List the available hyperparameters
You can explore the hyperparameters of a specific machine learning algorithm by using the .explainParams() method on a model. For example, if we want to train a linear regression model lr, we can use the following command to view the available hyperparameters:
Python
print(lr.explainParams())
The .explainParams() method will return a list of hyperparameters you can choose from, including the name of the hyperparameter, a description, and the default value. Three of the hyperparamaters available for the linear regression model are:
maxIter: max number of iterations (>= 0). (default: 100)
fitIntercept: whether to fit an intercept term. (default: True)
standardization: whether to standardize the training features before fitting the model. (default: True)
Set up the search space and sampling method
After you select the hyperparameters, you can use ParamGridBuilder() to specify the search space. The search space is the range of values of the hyperparameters you want to try out. You can then specify how you want to choose values from that search space to train individual models with which is known as the sampling method. The most straight-forward sampling method is known as grid sampling. The grid sampling method tries all possible combinations of values for the hyperparameters listed.
By default, the individual models will be trained in serial. It is possible to train models with different hyperparamater values in parallel. You can find more information on setting up the parameter grid in the documentation here.
Since grid search works through exhaustively building a model for each combination of hyperparameters, it quickly becomes a lot of different unique combinations. As each model training can consume a lot of compute power, be careful with the configuration you set up.
If we continue the example with the linear regression model lr, the following code shows how to set up a grid search to try out all possible combinations of parameters:
Python
from pyspark.ml.tuning import ParamGridBuilder
paramGrid = (ParamGridBuilder()
.addGrid(lr.maxIter, [1, 10, 100])
.addGrid(lr.fitIntercept, [True, False])
.addGrid(lr.standardization, [True, False])
.build()
)
Run code and invoke automated MLflow
To test how the model performs and to generate evaluation metrics, you can use a test dataset. If you want to train multiple models on the same training dataset and the same test dataset, you can use the TrainValidationSplit method to run your code, build the models, and log them automatically with MLflow.
In case you want to take extra measures to prevent overfitting, you can use the CrossValidator method to train the models with different training datasets for each model and different test datasets to calculate the evaluation metrics.
To build the models for the linear regression model lr used in the examples above, you can create a RegressionEvaluator() to evaluate the grid search experiments, which will help decide which model is best. The settings for the hyperparameter tuning experiment can be set by using the CrossValidator() method as is done in the example below.
Python
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import CrossValidator
evaluator = RegressionEvaluator(
labelCol = “medv“,
predictionCol = “prediction“
)
cv = CrossValidator(
estimator = pipeline, # Estimator (individual model or pipeline)
estimatorParamMaps = paramGrid, # Grid of parameters to try (grid search)
evaluator=evaluator, # Evaluator
numFolds = 3, # Set k to 3
seed = 42 # Seed to sure our results are the same if ran again
)
cvModel = cv.fit(trainDF)
Once all models have been trained, you can get the best model with the following code:
Python
bestModel = cvModel.bestModel
Alternatively, you can look at all models you trained through the UI of MLflow. Just remember that there will be a parent run for the complete experiment and child runs for each individual model that has been trained.
https://learn.microsoft.com/en-us/azure/databricks/machine-learning/automl-hyperparam-tuning/mllib-mlflow-integration",""
